# Propositions of papers for the year 2023

Please remove the paper from the list below and add it to the schedule in [README.md](https://github.com/ZILiAT-NASK/reading-club/blob/master/README.md) when you decide to present it.

## Computer Vision
- [Segment Everything Everywhere All at Once](https://arxiv.org/abs/2304.06718)
- [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)
- [Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold](https://arxiv.org/abs/2305.10973v1)
- [Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution](https://arxiv.org/abs/2307.06304)
- [MVDream: Multi-view Diffusion for 3D Generation](https://arxiv.org/abs/2308.16512)
- [SAM-Med2D](https://arxiv.org/abs/2308.16184v1)
- [Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language](https://arxiv.org/pdf/abs/2306.16410)

## Natural Language Processing
- [Evaluating Verifiability in Generative Search Engines](https://arxiv.org/abs/2304.09848)
- [Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/abs/2304.15004)
- [Unlimiformer: Long-Range Transformers with Unlimited Length Input](https://arxiv.org/abs/2305.01625)
- [Evidence of Meaning in Language Models Trained on Programs](https://arxiv.org/abs/2305.11169)
- [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining](https://arxiv.org/abs/2305.10429)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [The False Promise of Imitating Proprietary LLMs](https://arxiv.org/abs/2305.15717)
- [The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python](https://arxiv.org/abs/2305.15507)
- [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)
- [Fine-Tuning Language Models with Just Forward Passes](https://arxiv.org/abs/2305.17333)
- [FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios](https://arxiv.org/abs/2307.13528v2)

## Audio Processing
- [Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale](https://research.facebook.com/publications/voicebox-text-guided-multilingual-universal-speech-generation-at-scale)
- [AudioPaLM: A Large Language Model That Can Speak and Listen](https://arxiv.org/abs/2306.12925v1)

## Reinforcement Learning
- [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442)
- [Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2307.15217)
- [AgentBench: Evaluating LLMs as Agents](https://arxiv.org/abs/2308.03688v1)

## XAI
- [Model evaluation for extreme risks](https://arxiv.org/abs/2305.15324)
- [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)

## Multimodal Learning
- [Generative Disco: Text-to-Video Generation for Music Visualization](https://arxiv.org/abs/2304.08551)
- [Generative Pretraining in Multimodality](https://arxiv.org/abs/2307.05222v1)
- [Meta-Transformer: A Unified Framework for Multimodal Learning](https://arxiv.org/abs/2307.10802)
- [SeamlessM4Tâ€”Massively Multilingual & Multimodal Machine Translation](https://ai.meta.com/research/publications/seamless-m4t)
- [Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning](https://ai.meta.com/research/publications/scaling-autoregressive-multi-modal-models-pretraining-and-instruction-tuning)

## Neural Networks Fundamentals

- [Automatic Gradient Descent: Deep Learning without Hyperparameters](https://arxiv.org/abs/2304.05187)
- [Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training](https://arxiv.org/abs/2305.14342)
- [Scaling MLPs: A Tale of Inductive Bias](https://arxiv.org/abs//2306.13575)
- [Bayesian Flow Networks](https://arxiv.org/abs/2308.07037)
- [Curriculum learning](https://www.researchgate.net/publication/221344862_Curriculum_learning)


